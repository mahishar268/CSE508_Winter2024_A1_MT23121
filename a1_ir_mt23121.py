# -*- coding: utf-8 -*-
"""A1_IR_MT23121.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1diiRk9CQk0LdBQvz-FFLhATf9rS9FOsV
"""

from google.colab import drive
drive.mount('/content/drive')

pip install beautifulsoup4

import os
import nltk
from nltk.tokenize import word_tokenize
from nltk.corpus import stopwords
import string
from bs4 import BeautifulSoup
import re

# Download NLTK resources
nltk.download('punkt')
nltk.download('stopwords')

from bs4 import BeautifulSoup
import string
from nltk.tokenize import word_tokenize
from nltk.corpus import stopwords

def preprocess_text(text):
    # Lowercase the text
    text = text.lower()

    # Remove HTML tags using BeautifulSoup
    text = BeautifulSoup(text, "html.parser").get_text()

    # Tokenization
    tokens = word_tokenize(text)

    # Remove stopwords
    stop_words = set(stopwords.words('english'))
    tokens = [token for token in tokens if token not in stop_words]

    # Remove punctuations and ellipsis
    tokens = [token for token in tokens if token not in string.punctuation and token.strip() != "...."]

    # Join tokens back into text
    processed_text = ' '.join(tokens)

    return processed_text


# Process all files in the dataset
dataset_path = "/content/drive/MyDrive/text_files"  # Replace "path/to/dataset" with the actual path to your dataset directory
preprocessed_path = "/content/drive/MyDrive/preprocessed_files"  # New directory for preprocessed files

# Create directory if it doesn't exist
os.makedirs(preprocessed_path, exist_ok=True)

for filename in os.listdir(dataset_path):
    if filename.endswith(".txt"):
        file_path = os.path.join(dataset_path, filename)

        # Preprocess the file
        with open(file_path, "r") as file:
            original_text = file.read()
            processed_text = preprocess_text(original_text)

        # Save preprocessed text to the new directory
        preprocessed_file_path = os.path.join(preprocessed_path, "preprocessed_" + filename)
        with open(preprocessed_file_path, "w") as file:
            file.write(processed_text)

import os
import random

# Function to randomly select 5 files from a directory
def select_random_files(directory):
    file_list = os.listdir(directory)
    selected_files = random.sample(file_list, min(5, len(file_list)))  # Ensure we don't try to select more files than exist
    return selected_files

# Function to print contents of files
def print_file_contents(text_files_directory, preprocessed_files_directory, file_names):
    for file_name in file_names:
        text_file_path = os.path.join(text_files_directory, file_name)
        print("File:", file_name)
        if os.path.exists(text_file_path):
            with open(text_file_path, "r") as file:
                print("Original Text:")
                print(file.read())

            preprocessed_file_name = "preprocessed_" + file_name
            preprocessed_file_path = os.path.join(preprocessed_files_directory, preprocessed_file_name)
            print("Preprocessed Text:")
            if os.path.exists(preprocessed_file_path):
                with open(preprocessed_file_path, "r") as file:
                    print(file.read())
            else:
                print("Preprocessed file not found:", preprocessed_file_path)
        else:
            print("Text file not found:", text_file_path)
        print()

# Replace 'text_files_directory' and 'preprocessed_files_directory' with the paths to the respective directories
text_files_directory = "/content/drive/MyDrive/text_files"
preprocessed_files_directory = "/content/drive/MyDrive/preprocessed_files"
selected_files = select_random_files(text_files_directory)
print_file_contents(text_files_directory, preprocessed_files_directory, selected_files)

import os
import re
import pickle
from collections import defaultdict
from tqdm import tqdm

def tokenize(text):
    # Tokenization using a simple regular expression
    words = re.findall(r'\b\w+\b', text.lower())
    return words

def build_inverted_index(directory):
    inverted_index = defaultdict(list)
    file_count = 0

    for filename in tqdm(os.listdir(directory)):
        file_count += 1
        with open(os.path.join(directory, filename), 'r', encoding='utf-8') as file:
            content = file.read()
            tokens = tokenize(content)
            for token in tokens:
                inverted_index[token].append(filename)

    return inverted_index, file_count

def save_model(model, output_file):
    with open(output_file, 'wb') as file:
        pickle.dump(model, file)

def load_model(input_file):
    with open(input_file, 'rb') as file:
        model = pickle.load(file)
    return model

# Directory containing the files
directory = '/content/drive/MyDrive/preprocessed_files'

# Output file to save the model
output_file = 'inverted_index.pkl'

# Build inverted index
inverted_index, file_count = build_inverted_index(directory)

# Save the model
save_model((inverted_index, file_count), output_file)

print("Inverted index created and saved successfully!")

# Load the model from file
loaded_model = load_model(output_file)
loaded_inverted_index, loaded_file_count = loaded_model

# Display the inverted index
print("\nInverted Index:")
for term, postings in loaded_inverted_index.items():
    print(f"{term}: {postings}")

print(f"\nTotal files indexed: {loaded_file_count}")

import pickle

# Load the model from the pickle file
with open("/content/inverted_index.pkl", "rb") as file:
    loaded_model = pickle.load(file)

# Extract the inverted index and file count from the loaded model
loaded_inverted_index, loaded_file_count = loaded_model

# Save the inverted index as a dictionary file
with open("inverted_index_dictionary.txt", "w") as file:
    # Write each term and its postings list to the file
    for term, postings in loaded_inverted_index.items():
        file.write(f"{term}: {postings}\n")

print("Inverted index saved as dictionary file.")

import nltk
from nltk.tokenize import word_tokenize
from nltk.corpus import stopwords
import string
import re

def preprocess_query(query):
    # Lowercase the text
    query = query.lower()

    # Tokenization
    tokens = word_tokenize(query)

    # Remove stopwords
    stop_words = set(stopwords.words('english'))
    tokens = [token for token in tokens if token not in stop_words]

    # Remove punctuations
    tokens = [token for token in tokens if token not in string.punctuation]

    # Remove blank space tokens
    tokens = [token for token in tokens if token.strip()]

    return tokens

def AND(term1_docs, term2_docs):
    return list(set(term1_docs).intersection(set(term2_docs)))

def OR(term1_docs, term2_docs):
    return list(set(term1_docs).union(set(term2_docs)))

def AND_NOT(term1_docs, term2_docs):
    return list(set(term1_docs).difference(set(term2_docs)))

def OR_NOT(term1_docs, term2_docs, all_docs):
    return list(set(all_docs).difference(AND_NOT(all_docs, term1_docs)))

def process_query(query_tokens, operations, inverted_index):
    num_docs = None
    doc_names = None

    result = inverted_index.get(query_tokens[0], [])

    i = 1
    while i < len(query_tokens):
        operation = operations[i - 1]
        term = query_tokens[i]

        postings = inverted_index.get(term, [])

        if operation == "AND":
            result = AND(result, postings)
        elif operation == "OR":
            result = OR(result, postings)
        elif operation == "AND_NOT":
            result = AND_NOT(result, postings)
        elif operation == "OR_NOT":
            all_docs = list(inverted_index.keys())
            result = OR_NOT(result, postings, all_docs)
        else:
            raise ValueError(f"Invalid operation: {operation}")

        i += 1

    num_docs = len(result)
    doc_names = result

    return num_docs, doc_names

# Load inverted index from file
def load_index_from_file(filename):
    index = {}
    with open(filename, "r") as file:
        for line in file:
            term, postings = line.strip().split(": ")
            postings = postings.strip("[]").split(", ")
            index[term] = postings
    return index

# Take user input for the number of queries
num_queries = int(input("Enter the number of queries: "))

# Load inverted index from file
inverted_index = load_index_from_file("inverted_index_dictionary.txt")

# Process each query
for i in range(num_queries):
    query = input(f"Enter query {i + 1}: ")
    preprocessed_query = preprocess_query(query)
    operations = prompt_for_operations(preprocessed_query)

    # Process the query
    num_docs, doc_names = process_query(preprocessed_query, operations, inverted_index)

    # Print the result
    print(f"Number of documents for query {i + 1}: {num_docs}")
    print(f"Names of documents for query {i + 1}: {doc_names}")

import os

def tokenize(text):
    # Tokenize the text into words
    words = text.split()
    # Remove punctuation marks and convert to lowercase
    words = [word.strip().lower().strip(string.punctuation) for word in words]
    # Remove empty strings
    words = [word for word in words if word]
    return words

def build_positional_index(dataset_path):
    positional_index = {}

    # Iterate over each file in the dataset
    for filename in os.listdir(dataset_path):
        if filename.endswith('.txt'):
            file_path = os.path.join(dataset_path, filename)
            with open(file_path, 'r', encoding='utf-8') as file:
                text = file.read()
                tokens = tokenize(text)
                # Update positional index for each token
                for position, token in enumerate(tokens):
                    if token not in positional_index:
                        positional_index[token] = {}
                    if filename not in positional_index[token]:
                        positional_index[token][filename] = []
                    # Store the position of the token in the document
                    positional_index[token][filename].append(position)

    return positional_index

# Define the path to the dataset obtained from Q1
dataset_path = "/content/drive/MyDrive/preprocessed_files"

# Build the positional index
positional_index = build_positional_index(dataset_path)

# Print the positional index
for term, postings in positional_index.items():
    print(f"Term: {term}")
    for document, positions in postings.items():
        print(f"  Document: {document}")
        print(f"    Positions: {positions}")

import pickle

def save_positional_index_as_pickle(positional_index, filename):
    with open(filename, 'wb') as file:
        pickle.dump(positional_index, file)

def save_positional_index_as_text(positional_index, filename):
    with open(filename, 'w', encoding='utf-8') as file:
        for term, postings in positional_index.items():
            file.write(f"Term: {term}\n")
            for document, positions in postings.items():
                file.write(f"  Document: {document}\n")
                file.write(f"    Positions: {positions}\n")

# Define filenames for saving
pickle_filename = "positional_index.pkl"
text_filename = "positional_index.txt"

# Save as pickle
save_positional_index_as_pickle(positional_index, pickle_filename)

# Save as text file
save_positional_index_as_text(positional_index, text_filename)

print("Positional index saved as pickle and text files.")

from nltk.tokenize import word_tokenize
from nltk.corpus import stopwords
import string

def preprocess_query(query):
    # Lowercase the text
    query = query.lower()

    # Tokenization
    tokens = word_tokenize(query)

    # Remove stopwords
    stop_words = set(stopwords.words('english'))
    tokens = [token for token in tokens if token not in stop_words]

    # Remove punctuations
    tokens = [token for token in tokens if token not in string.punctuation]

    # Remove blank space tokens
    tokens = [token for token in tokens if token.strip()]

    return tokens

def read_positional_index_from_text(filename):
    positional_index = {}
    with open(filename, 'r', encoding='utf-8') as file:
        lines = file.readlines()
        term = None
        document = None
        for line in lines:
            line = line.strip()
            if line.startswith("Term:"):
                term = line.split(":")[1].strip()
                positional_index[term] = {}
            elif line.startswith("Document:"):
                document = line.split(":")[1].strip()
            elif line.startswith("Positions:"):
                positions_line = line.split(":")[1].strip()
                positions = list(map(int, positions_line[1:-1].split(", ")))
                positional_index[term][document] = positions
    return positional_index


def execute_queries(positional_index, queries):
    results = []
    for query in queries:
        query_terms = preprocess_query(query)
        retrieved_documents = None
        for term in query_terms:
            if term in positional_index:
                if retrieved_documents is None:
                    retrieved_documents = set(positional_index[term].keys())
                else:
                    retrieved_documents = retrieved_documents.intersection(positional_index[term].keys())
        if retrieved_documents is not None:
            results.append(len(retrieved_documents))
            results.extend(list(retrieved_documents))
        else:
            results.append(0)
    return results

# Read positional index from text file
positional_index = read_positional_index_from_text("positional_index.txt")

# Read input queries
N = int(input("Enter the number of queries: "))
queries = [input("Enter phrase query: ") for _ in range(N)]

# Execute queries
output = execute_queries(positional_index, queries)

# Print output in specified format
for i in range(N):
    print(f"Number of documents retrieved for query {queries[i]} using positional index:")
    print(output[i])
    if output[i] > 0:
        print("Names of documents retrieved for query {} using positional index:".format(queries[i]))
        for j in range(output[i]):
            print(output[i + j + 1])
        i += output[i]